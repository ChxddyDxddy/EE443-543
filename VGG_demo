{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChxddyDxddy/EE443-543/blob/main/VGG_demo\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3_DPvpEEz4Gg",
      "metadata": {
        "id": "3_DPvpEEz4Gg"
      },
      "source": [
        "Compact VGG-11 for CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0slSJnrQ2bNc",
      "metadata": {
        "id": "0slSJnrQ2bNc"
      },
      "source": [
        "The paper investigates how increasing the depth of convolutional neural networks affects accuracy in large-scale image recognition, specifically on the ImageNet (ILSVRC) dataset. The authors propose a family of architectures called VGG networks that use a simple but strictly consistent design: stack many 3×3 convolution layers, increase depth systematically, and downsample using max-pooling. They explore models ranging from shallow to very deep, ultimately demonstrating that networks with 16–19 weight layers significantly outperform previous architectures.\n",
        "\n",
        "The authors focus exclusively on the impact of depth by keeping all other architectural choices fixed. Using only 3×3 convolutions allows them to deepen the networks without excessively increasing computation or parameters. The experiments show that deeper models produce substantially better results on classification and localization tasks, achieving state of the art performance on ILSVRC 2014. Their approach won 1st place in localization and 2nd place in classification in that challenge.\n",
        "\n",
        "The paper also highlights that these deep representations generalize well to other vision tasks and datasets, outperforming or matching more complex pipelines when used as feature extractors. The results reinforce the central finding: increasing network depth, while using small convolution filters, substantially improves visual recognition performance. The authors also released their top-performing models publicly to support further research.\n",
        "\n",
        "Finally, the paper concludes that simply making ConvNets deeper,   without changing the basic architecture, yields strong improvements and reinforces the importance of depth in learning effective visual representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bu5a17ga0MGh",
      "metadata": {
        "id": "bu5a17ga0MGh"
      },
      "outputs": [],
      "source": [
        "import torch # Main PyTorch library\n",
        "import torch.nn as nn # Neural network building blocks\n",
        "import torch.nn.functional as F # Common functional operations (e.g., ReLU, conv)\n",
        "\n",
        "# Define the Compact VGG-11 architecture tailored for CIFAR-10\n",
        "class CompactVGG11_CIFAR10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # FEATURE EXTRACTOR: 8 convolutional layers + 5 max-pool layers\n",
        "        # This mirrors the structure of VGG-11 while reducing channel counts.\n",
        "        self.features = nn.Sequential(\n",
        "\n",
        "            # ------------------ Block 1 ------------------\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # 3→32 channel conv on 32×32 input\n",
        "            nn.BatchNorm2d(32),                          # BN stabilizes and speeds training\n",
        "            nn.ReLU(inplace=True),                       # Nonlinearity\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Downsample: 32→16 resolution\n",
        "\n",
        "            # ------------------ Block 2 ------------------\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 32→64 feature channels\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # 16→8 resolution\n",
        "\n",
        "            # ------------------ Block 3 ------------------\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# Increase depth: 64→128\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# Second conv in block\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # 8→4 resolution\n",
        "\n",
        "            # ------------------ Block 4 ------------------\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# 128→256 channels\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# Deepen representation\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # 4→2 resolution\n",
        "\n",
        "            # ------------------ Block 5 ------------------\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# More depth without increasing width\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # 2→1 resolution\n",
        "        )\n",
        "\n",
        "        # After 5 pools: 32→16→8→4→2→1, so final feature map is 256×1×1\n",
        "\n",
        "        # CLASSIFIER HEAD: replaces huge VGG FC layers with lightweight layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 256),   # Flattened feature vector: 256 → 256\n",
        "            nn.ReLU(inplace=True), # Activation\n",
        "            nn.Dropout(0.5),       # Regularization to prevent overfitting\n",
        "            nn.Linear(256, num_classes) # Final 10-class output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through convolutional blocks\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Flatten: convert (batch, 256, 1, 1) → (batch, 256)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Pass flattened features through the classifier\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        # Return raw logits (softmax handled by loss function)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "osT6ji_q6UKH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osT6ji_q6UKH",
        "outputId": "3b8342d8-e2b3-419f-d982-effd9b64cdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup (run first)\n",
        "\n",
        "# This installs PyTorch and torchvision inside Colab in case they're not already there.\n",
        "!pip install torch torchvision --quiet\n",
        "\n",
        "import time\n",
        "import torch                                # import the main PyTorch package.\n",
        "import torch.nn as nn                       # import the neural network module, which has layers like Linear, Conv2d, etc.\n",
        "import torch.nn.functional as F             # import the functional API for operations like relu.\n",
        "import torch.optim as optim                 # import the optimizers module, which has SGD, Adam, etc.\n",
        "from torch.utils.data import DataLoader     # import DataLoader to create mini-batches from the dataset.\n",
        "import torchvision                          # import torchvision, which gives me common datasets and transforms.\n",
        "import torchvision.transforms as transforms # import the transforms module to apply data augmentation and normalization.\n",
        "import matplotlib.pyplot as plt             # import matplotlib so we can create plots later for the results.\n",
        "\n",
        "# choose to run on GPU if it's available, otherwise it fall back to CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# print which device It's using so we know whether It's on CPU or GPU.\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Data: CIFAR-10 with basic augmentation and normalization\n",
        "\n",
        "# define the transformations applied to each training image.\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # randomly crop the image with padding to add spatial jitter.\n",
        "    transforms.RandomHorizontalFlip(),     # randomly flip the image horizontally to augment the data.\n",
        "    transforms.ToTensor(),                 # convert the image from PIL format to a PyTorch tensor.\n",
        "    transforms.Normalize(                  # normalize each channel using the CIFAR-10 mean and std.\n",
        "        (0.4914, 0.4822, 0.4465),          # Mean for R, G, B channels.\n",
        "        (0.2470, 0.2435, 0.2616)           # Standard deviation for R, G, B channels.\n",
        "    ),\n",
        "])\n",
        "\n",
        "# define the transformations for the test set (no augmentation, just normalization).\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),                 # convert the test image to a tensor.\n",
        "    transforms.Normalize(                  # normalize using the same statistics as the training set.\n",
        "        (0.4914, 0.4822, 0.4465),\n",
        "        (0.2470, 0.2435, 0.2616)\n",
        "    ),\n",
        "])\n",
        "\n",
        "# load the CIFAR-10 training set from disk (downloading it if needed) with the training transforms.\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',                         # This is the directory where the data will be stored.\n",
        "    train=True,                            # indicate that we want the training split.\n",
        "    download=True,                         # allow it to download the dataset if it's not already there.\n",
        "    transform=transform_train              # apply the training transformations defined above.\n",
        ")\n",
        "\n",
        "# load the CIFAR-10 test set with the test transforms.\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',                         # Same root directory for the data.\n",
        "    train=False,                           # indicate that we want the test split.\n",
        "    download=True,                         # Again, download if necessary.\n",
        "    transform=transform_test               # apply the test transformations.\n",
        ")\n",
        "\n",
        "# wrap the training dataset in a DataLoader to get mini-batches and shuffling.\n",
        "trainloader = DataLoader(\n",
        "    trainset,                              # The dataset to load from.\n",
        "    batch_size=128,                        # The number of images per batch.\n",
        "    shuffle=True,                          # shuffle the data every epoch for better training.\n",
        "    num_workers=2                          # Number of worker processes for loading data in parallel.\n",
        ")\n",
        "\n",
        "# wrap the test dataset in a DataLoader as well, typically without shuffling.\n",
        "testloader = DataLoader(\n",
        "    testset,                               # The dataset to load from.\n",
        "    batch_size=256,                        # A larger batch size is fine for evaluation.\n",
        "    shuffle=False,                         # don't need to shuffle test data.\n",
        "    num_workers=2                          # Same number of workers for loading.\n",
        ")\n",
        "\n",
        "# Compact VGG-11 model: fewer channels, lighter classifier\n",
        "\n",
        "# define the compact VGG-11 model designed for CIFAR-10.\n",
        "class CompactVGG11_CIFAR10(nn.Module):\n",
        "    # The constructor takes an optional number of output classes, defaulting to 10 for CIFAR-10.\n",
        "    def __init__(self, num_classes=10):\n",
        "        # initialize the parent nn.Module class.\n",
        "        super().__init__()\n",
        "\n",
        "        # define the convolutional feature extractor using nn.Sequential.\n",
        "        self.features = nn.Sequential(\n",
        "            # -------- Block 1 --------\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # First conv: 3 input channels → 32 output channels.\n",
        "            nn.BatchNorm2d(32),                          # BatchNorm to stabilize training for 32 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation applied in-place to save memory.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Max pooling halves the spatial size: 32x32 → 16x16.\n",
        "\n",
        "            # -------- Block 2 --------\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Second conv: 32 → 64 channels.\n",
        "            nn.BatchNorm2d(64),                          # BatchNorm for the 64-channel feature map.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 16x16 → 8x8.\n",
        "\n",
        "            # -------- Block 3 --------\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# Third conv: 64 → 128 channels.\n",
        "            nn.BatchNorm2d(128),                         # BatchNorm for the 128-channel feature map.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# Another conv at 128 channels to deepen features.\n",
        "            nn.BatchNorm2d(128),                         # BatchNorm again for stability.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 8x8 → 4x4.\n",
        "\n",
        "            # -------- Block 4 --------\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# Fourth conv: 128 → 256 channels.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm for 256 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# Another 256-channel conv for more capacity.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm again.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 4x4 → 2x2.\n",
        "\n",
        "            # -------- Block 5 --------\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# Fifth block conv: keep 256 channels.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm for 256 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# Another conv at 256 channels.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm again.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Final pooling: 2x2 → 1x1.\n",
        "        )\n",
        "\n",
        "        # After all pooling operations, the feature map is 256 channels of size 1x1.\n",
        "\n",
        "        # define the fully connected classifier part.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 256),                         # map the 256-dim feature vector to 256 hidden units.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation for the hidden layer.\n",
        "            nn.Dropout(0.5),                             # Dropout to reduce overfitting with 50% drop rate.\n",
        "            nn.Linear(256, num_classes)                  # Final linear layer maps to the number of classes.\n",
        "        )\n",
        "\n",
        "    # define the forward pass, which describes how data flows through the model.\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)                             # pass the input through the convolutional feature extractor.\n",
        "        x = x.view(x.size(0), -1)                        # flatten the feature map to a 2D tensor: (batch_size, 256).\n",
        "        x = self.classifier(x)                           # pass the flattened features through the classifier.\n",
        "        return x                                         # return the raw logits (no softmax here).\n",
        "\n",
        "\n",
        "# Standard VGG-11-style model: wider version for comparison\n",
        "\n",
        "# This class defines a larger VGG-11-style model with more channels in each layer.\n",
        "class StandardVGG11_CIFAR10(nn.Module):\n",
        "    # Again, allow specifying the number of classes, defaulting to 10.\n",
        "    def __init__(self, num_classes=10):\n",
        "        # Initialize the base class.\n",
        "        super().__init__()\n",
        "\n",
        "        # define a deeper feature extractor using higher channel counts.\n",
        "        self.features = nn.Sequential(\n",
        "            # -------- Block 1 --------\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # First conv: 3 → 64 channels.\n",
        "            nn.BatchNorm2d(64),                          # BatchNorm for 64 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 32x32 → 16x16.\n",
        "\n",
        "            # -------- Block 2 --------\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# Conv: 64 → 128 channels.\n",
        "            nn.BatchNorm2d(128),                         # BatchNorm for 128 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 16x16 → 8x8.\n",
        "\n",
        "            # -------- Block 3 --------\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),# Conv: 128 → 256 channels.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm for 256 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),# Another conv at 256 channels.\n",
        "            nn.BatchNorm2d(256),                         # BatchNorm again.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 8x8 → 4x4.\n",
        "\n",
        "            # -------- Block 4 --------\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),# Conv: 256 → 512 channels.\n",
        "            nn.BatchNorm2d(512),                         # BatchNorm for 512 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),# Another conv at 512 channels.\n",
        "            nn.BatchNorm2d(512),                         # BatchNorm again.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Pooling: 4x4 → 2x2.\n",
        "\n",
        "            # -------- Block 5 --------\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),# Conv keeps 512 channels.\n",
        "            nn.BatchNorm2d(512),                         # BatchNorm for 512 channels.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),# Another 512-channel conv.\n",
        "            nn.BatchNorm2d(512),                         # BatchNorm again.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),       # Final pooling: 2x2 → 1x1.\n",
        "        )\n",
        "\n",
        "        # After all pooling, the feature map is 512 channels with spatial size 1x1.\n",
        "\n",
        "        # define the classifier for the larger model.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),                         # Map 512 features to a 512-unit hidden layer.\n",
        "            nn.ReLU(inplace=True),                       # ReLU activation.\n",
        "            nn.Dropout(0.5),                             # Dropout to help with overfitting.\n",
        "            nn.Linear(512, num_classes)                  # Final output layer: 512 → num_classes.\n",
        "        )\n",
        "\n",
        "    # define the forward pass for the larger model.\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)                             # Pass input through all convolutional blocks.\n",
        "        x = x.view(x.size(0), -1)                        # Flatten to (batch_size, 512).\n",
        "        x = self.classifier(x)                           # Pass through the classifier.\n",
        "        return x                                         # Return logits.\n",
        "\n",
        "\n",
        "# Training and evaluation helper functions\n",
        "\n",
        "# This function trains the model for one epoch and measures accuracy and time.\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()                                       # set the model to training mode (enables dropout, etc.).\n",
        "    correct = 0                                         # initialize a counter for correct predictions.\n",
        "    total = 0                                           # initialize a counter for total samples.\n",
        "    running_loss = 0.0                                  # initialize the accumulated loss.\n",
        "\n",
        "    start_time = time.time()                            # record the start time to measure epoch duration.\n",
        "    for inputs, targets in loader:                      # loop over each mini-batch in the DataLoader.\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # move data and labels to the selected device.\n",
        "\n",
        "        optimizer.zero_grad()                           # clear previous gradients before the next backward pass.\n",
        "        outputs = model(inputs)                         # run a forward pass to get model predictions.\n",
        "        loss = criterion(outputs, targets)              # compute the loss between predictions and true labels.\n",
        "        loss.backward()                                 # backpropagate the gradients through the network.\n",
        "        optimizer.step()                                # update the model parameters using the optimizer.\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)    # accumulate the loss scaled by batch size.\n",
        "        _, predicted = outputs.max(1)                   # take the class with the highest logit as the prediction.\n",
        "        total += targets.size(0)                        # update the total number of samples.\n",
        "        correct += predicted.eq(targets).sum().item()   # update the count of correct predictions.\n",
        "\n",
        "    epoch_time = time.time() - start_time               # compute how long this epoch took.\n",
        "    avg_loss = running_loss / total                     # compute the average loss over the epoch.\n",
        "    acc = 100.0 * correct / total                       # compute the training accuracy in percent.\n",
        "    return avg_loss, acc, epoch_time                    # return loss, accuracy, and epoch time.\n",
        "\n",
        "# use torch.no_grad to turn off gradient tracking during evaluation.\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()                                        # set the model to evaluation mode (disables dropout).\n",
        "    correct = 0                                         # initialize correct prediction counter.\n",
        "    total = 0                                           # initialize total sample counter.\n",
        "    running_loss = 0.0                                  # initialize accumulated loss.\n",
        "\n",
        "    for inputs, targets in loader:                      # loop over the evaluation DataLoader.\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # move data and labels to the device.\n",
        "        outputs = model(inputs)                         # run a forward pass.\n",
        "        loss = criterion(outputs, targets)              # compute the loss.\n",
        "        running_loss += loss.item() * inputs.size(0)    # accumulate the loss.\n",
        "        _, predicted = outputs.max(1)                   # get the predicted class indices.\n",
        "        total += targets.size(0)                        # update the total number of samples.\n",
        "        correct += predicted.eq(targets).sum().item()   # count how many predictions are correct.\n",
        "\n",
        "    avg_loss = running_loss / total                     # compute the average test loss.\n",
        "    acc = 100.0 * correct / total                       # compute the test accuracy in percent.\n",
        "    return avg_loss, acc                                # return test loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l2gD9C6V6dRD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "l2gD9C6V6dRD",
        "outputId": "fbac4630-8e9e-4df2-b67b-6a20a8810cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Compact VGG-11 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2009913604.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Run both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompactVGG11_CIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Compact VGG-11\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardVGG11_CIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Standard VGG-11\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2009913604.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model_class, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1158069689.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# run a forward pass to get model predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# compute the loss between predictions and true labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# backpropagate the gradients through the network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                \u001b[0;31m# update the model parameters using the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 2: Experiments\n",
        "\n",
        "num_epochs = 20  # increase for better final accuracy\n",
        "\n",
        "results = {}\n",
        "\n",
        "def run_experiment(model_class, name):\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    model = model_class().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 12], gamma=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    epoch_times = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    total_time = 0.0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc, epoch_time = train_one_epoch(model, trainloader, optimizer, criterion)\n",
        "        val_loss, val_acc = evaluate(model, testloader, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        total_time += epoch_time\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        print(f\"Epoch [{epoch:02d}/{num_epochs}]  \"\n",
        "              f\"Train Acc: {train_acc:6.2f}%  \"\n",
        "              f\"Val Acc: {val_acc:6.2f}%  \"\n",
        "              f\"Time: {epoch_time:5.2f}s\")\n",
        "\n",
        "    results[name] = {\n",
        "        \"train_accs\": train_accs,\n",
        "        \"val_accs\": val_accs,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"total_time\": total_time,\n",
        "    }\n",
        "\n",
        "# Run both models\n",
        "run_experiment(CompactVGG11_CIFAR10, \"Compact VGG-11\")\n",
        "run_experiment(StandardVGG11_CIFAR10, \"Standard VGG-11\")\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:16s} | best val acc = {v['best_val_acc']:.2f}% | total train time = {v['total_time']:.1f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YRIwma8I6hPk",
      "metadata": {
        "id": "YRIwma8I6hPk"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Visualization\n",
        "\n",
        "# --- 1) Accuracy vs epoch ---\n",
        "plt.figure(figsize=(8, 5))                      # creates a new figure and set a size that looks good in slides\n",
        "epochs = range(1, num_epochs + 1)               # makes a range object for the epoch numbers along the x-axis\n",
        "\n",
        "# loops through each model's results to plot their curves\n",
        "for name, data in results.items():\n",
        "    # plots the training accuracy curve with a dashed line and circle markers\n",
        "    plt.plot(\n",
        "        epochs,                                  # The x-axis is the epoch number.\n",
        "        data[\"train_accs\"],                      # The y-axis is the training accuracy for each epoch.\n",
        "        linestyle='--',                          # uses a dashed line style for training accuracy\n",
        "        marker='o',                              # uses circular markers for each epoch\n",
        "        label=f\"{name} - train\"                  # labels this line in the legend with the model name and 'train'\n",
        "    )\n",
        "\n",
        "    # plots the validation accuracy curve with a solid line and x markers\n",
        "    plt.plot(\n",
        "        epochs,                                  # Again, x-axis is epochs\n",
        "        data[\"val_accs\"],                        # Y-axis is the validation accuracy for each epoch\n",
        "        linestyle='-',                           # uses a solid line style for validation accuracy\n",
        "        marker='x',                              # uses 'x' markers for each epoch\n",
        "        label=f\"{name} - val\"                    # labels this line in the legend with the model name and 'val'\n",
        "    )\n",
        "\n",
        "plt.title(\"Accuracy vs Epoch on CIFAR-10\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True, linestyle=':')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 2) Time vs Best Accuracy scatter plot ---\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "# loops through each model's results again.\n",
        "for name, data in results.items():\n",
        "    # converts the total training time from seconds to minutes for the x-axis\n",
        "    x = data[\"total_time\"] / 60.0\n",
        "\n",
        "    # grabs the best validation accuracy for the y-axis\n",
        "    y = data[\"best_val_acc\"]\n",
        "\n",
        "    # plots a single scatter point for this model at (total_time_minutes, best_accuracy)\n",
        "    plt.scatter(x, y, s=80)\n",
        "\n",
        "    # annotates each point by writing the model name slightly to the right of the point\n",
        "    plt.text(x + 0.05, y, name, fontsize=10)\n",
        "\n",
        "plt.title(\"Time vs Best Accuracy on CIFAR-10\")\n",
        "plt.xlabel(\"Total Training Time (minutes)\")\n",
        "plt.ylabel(\"Best Validation Accuracy (%)\")\n",
        "plt.grid(True, linestyle=':')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4: Presentation DEMO\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# CIFAR-10 classes for display\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Simple transform (no cropping or flipping)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 test set BUT DO NOT use DataLoader\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# --- Get a single image (index 0) ---\n",
        "image, label = testset[5300]   # one image, one label\n",
        "\n",
        "# Convert tensor → numpy for showing\n",
        "img_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "# Undo CIFAR-10 normalization for display (if needed)\n",
        "# Here we didn't normalize, so nothing to undo.\n",
        "\n",
        "# --- Print what picture it is ---\n",
        "print(\"Label index:\", label)\n",
        "print(\"This picture is a:\", classes[label])\n",
        "\n",
        "# --- Show the picture ---\n",
        "plt.imshow(img_np)\n",
        "plt.title(f\"Label: {classes[label]}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ExEMQdI3wtT1",
        "outputId": "ea333bc9-0b58-46bf-b4ea-af9b8fa67e57"
      },
      "id": "ExEMQdI3wtT1",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label index: 9\n",
            "This picture is a: truck\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIfJJREFUeJzt3XuQnAWZ7/Gn7z3Tc79mZshkMkkgF8DE3FwMJiAQXFGhRBAWhVIpS60jVRYoHi9QZR05FgpYlLWwJQgeD3iBrIsBuawmugibEMSQBEOSSWaSmVzmPtPTPd3Tl/f8cQ7P2ZgAz8Ny2V2/nyr+GX7z5O3ut/vX72T6SSgIgkAAABCR8Dt9AACA/zgoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAX8h9Tb2yuhUEi++93vvmkzN2/eLKFQSDZv3vymzXy7rVu3Tk4//fR3+jDwXxilgDfNfffdJ6FQSLZt2/ZOH8pb4oEHHpA77rjjnT4M4C1FKQBGlAL+GlAKwFsgl8tJuVx+pw8DcKMU8LaamZmRb37zm7J8+XKpra2VVColZ599tmzatOlVv+f222+XOXPmSEVFhaxdu1Z27tx5Qmb37t1y6aWXSkNDgySTSVmxYoU88sgjr3s82WxWdu/eLcPDw6+ZW7dunTz66KPS19cnoVBIQqGQdHV1icj//7uKn/70p/L1r39dOjo6pLKyUiYnJ+Xmm2+WUCh0wrxXftTW29t73Nd//etfy9q1a6W6ulpqampk5cqV8sADD7zmsT355JNSWVkpV1xxhRSLxde9zcBrib7TB4C/LpOTk/LDH/5QrrjiCrn22mslnU7LPffcI+vXr5etW7fK0qVLj8v/+Mc/lnQ6LV/4whckl8vJ97//fTn33HNlx44d0traKiIiu3btkve+973S0dEhN954o6RSKfn5z38uF198sTz88MNyySWXvOrxbN26Vc455xy56aab5Oabb37V3Ne+9jWZmJiQ/v5+uf3220VEpKqq6rjMt771LYnH43L99ddLPp+XeDzuum/uu+8++dSnPiVLliyRr371q1JXVycvvPCCPP7443LllVee9Hs2btwol156qVx++eVy7733SiQScf2ZwF+iFPC2qq+vl97e3uNeMK+99lpZuHCh3HnnnXLPPfccl9+3b5/s3btXOjo6RETkwgsvlNWrV8t3vvMdue2220RE5LrrrpPOzk557rnnJJFIiIjI5z//eVmzZo185Stfec1SsDr//POlo6NDxsbG5KqrrjppJpfLybZt26SiosI9f2JiQr74xS/KqlWrZPPmzZJMJvX/vdo/ebJhwwb5+Mc/Ltdcc43cddddEg5z4Y9/P84ivK0ikYgWQrlcltHRUSkWi7JixQr54x//eEL+4osv1kIQEVm1apWsXr1aHnvsMRERGR0dld/+9rdy2WWXSTqdluHhYRkeHpaRkRFZv3697N27VwYGBl71eNatWydBELzmVYLV1Vdf/YYKQUTkqaeeknQ6LTfeeONxhSAiJ/3x04MPPiiXX365fPazn5W7776bQsCbhjMJb7v7779fzjzzTEkmk9LY2CjNzc3y6KOPysTExAnZBQsWnPC1U089VX8Wv2/fPgmCQL7xjW9Ic3Pzcf/ddNNNIiIyODj4lt6eV8ydO/cNf29PT4+IiOkzCAcOHJCrrrpKPvrRj8qdd9550tIA3ih+fIS31U9+8hO55ppr5OKLL5YbbrhBWlpaJBKJyC233KIvjB6v/IbP9ddfL+vXrz9pZv78+f+uY7Y62VXCq71gl0qlN/zntLW1SVtbmzz22GOybds2WbFixRueBfwlSgFvq4ceeki6u7tlw4YNx71gvvKu/i/t3bv3hK/t2bNHf/Onu7tbRERisZicd955b/4B/xtv5B15fX29iIiMj49LXV2dfr2vr++43Lx580REZOfOna9bYslkUjZu3CjnnnuuXHjhhfK73/1OlixZ4j424GT48RHeVq/8dsy//cvTLVu2yLPPPnvS/C9/+cvj/k5g69atsmXLFvnABz4gIiItLS2ybt06ufvuu+XIkSMnfP/Q0NBrHo/1V1JFRFKp1El/xPVaXnmx//3vf69fy2Qycv/99x+Xu+CCC6S6ulpuueUWyeVyx/2/k/1Fc21trTzxxBPS0tIi559//hu6ygJOhisFvOnuvfdeefzxx0/4+nXXXScXXXSRbNiwQS655BL54Ac/KAcOHJC77rpLFi9eLFNTUyd8z/z582XNmjXyuc99TvL5vNxxxx3S2NgoX/7ylzXzgx/8QNasWSNnnHGGXHvttdLd3S3Hjh2TZ599Vvr7+2X79u2veqzWX0kVEVm+fLn87Gc/ky996UuycuVKqaqqkg996EOv+T0XXHCBdHZ2yqc//Wm54YYbJBKJyL333ivNzc1y8OBBzdXU1Mjtt98un/nMZ2TlypVy5ZVXSn19vWzfvl2y2ewJJSIi0tTUJE899ZSsWbNGzjvvPHn66aeP+0t54A0JgDfJj370o0BEXvW/Q4cOBeVyOfj2t78dzJkzJ0gkEsGyZcuCjRs3BldffXUwZ84cnXXgwIFARIJbb701+N73vhfMnj07SCQSwdlnnx1s3779hD+7p6cn+OQnPxnMmjUriMViQUdHR3DRRRcFDz30kGY2bdoUiEiwadOmE7520003ve7tm5qaCq688sqgrq4uEBE93ldm/OIXvzjp9z3//PPB6tWrg3g8HnR2dga33Xab3lcHDhw4LvvII48EZ511VlBRURHU1NQEq1atCh588EH9/2vXrg2WLFly3Pfs27cvaGtrCxYtWhQMDQ297u0AXksoCF7ll6ABAH91+DsFAICiFAAAilIAAChKAQCgKAUAgKIUAADK/OG1hSvOcA1ubWk3Z6fGRlyza6qrXj/0/4xMjLlmj49kzdm66lrX7NXvWWXOVlb6tm1ueORhVz5RYX8/MDPt29NTUx+zz3b+mzANDfbZZyw+2zX7mk9c68r/adsWc/Zfnvc9Pr0H0+Zsbtr3W+XpKfs5Hgp8n29tqG0xZweHR12zY3Hfe9hyyf4v3wXi+1fyQo5DKUd9j09Dtf0cb6xPuGY/ueGZ181wpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAGVebBKP1rsGFwrT5mwo5FuAMzI6aM7W1jS5Zg/0DZuzp84/zTW7tXWWOfv005tds2vqfDtQcnn7fV5VXemaHYvZdyVNZXOu2fl8yJydzPS6Zvf29Lvyf3rxD+ZsPOq7D8OxIXN2Yijjmh3M2M+VkHMnkOcf9w2H7I+liEg04nsPm53J22dHfTueYp688613UIyYs1NTM77hBlwpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFDmz2p3zTnFNTgQ+/qCfLzRNTsStq9oCIV8vZdKpczZctnxmX4Ref65583ZdHrKNVtCvmNJJu2rKKTsyIpIKLCvAEjE7R/p/7/5uD0b+Nan7O3Z7MpnCvZjyWQnXLNHhxzPn4zvPgw7VldEI75VFCNj9jUxpaJvhUYu61vpEI7an/vlwHcspRn7uRWP+lbQBPbTSjIF1lwAAN5ClAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAAZV5S09Rq38UiIpJJF8zZrs55rtlXXnqZOXv///6pa3bP/v3m7NDQUdfsWS0t5mzn7C7X7O27t7nyrc015mwQjrlmh8L2xz6XT7pmp8cz5uyh8ohrdkPrn1z5ZGWXOVssNbtmV1amzdnshG/vVaGYNWejMef7xrB9T1YsZt+RJSJSKvp2cEUi9vmRsG/HU+DYBxaO+Y67ULA/nkHA7iMAwFuIUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACj7mos2++oCEZFCMG7OnnXW37hm9x06Zs6OjtnXBYiIdM7pNGe379jpmv2Jv/s7c/bpZ/7gmi0R36qDwcFJc7ah2bfmojxtzwbFomt2ZbV9LYZjW4CIiIyM+lZuVFXY1yiUKupcs6tTDeZsunLQNTvkWC0SBL4VDeWy/U4v5H2rc2LRuDNvX11RLvpOloqE/T7MiX2tiIiIZ3NFNJRwzbbgSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAMq8vGVisuwanEq1mrOntHe4Zm/8p0fN2erqlGu2lGvM0XedvtA1es3as83ZHS/59irN7pjrypfyGXM2IvY9SSIiY4W8OZusrnDNTiXtu15a27pdsxvr57nyvb1bzNly2HeOVycazdn8TL9rdl2tfXZ2yrc7TML295mVVdWu0RnnsURC9p1dpaJvx1Mkbt+VFPK9dErZcR/GEm/++3quFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAo85qLwSP2tQgiItUVcXO2raXJNTuZqjRnu5vs6zZERH77zwPm7LkXXO6aPRmxH8vpXYtcszuPZl35qkr7+4Ff9u13zT46ab8PK6vs54mISDkxbc7GK31rEWLS58rPiP08rJ97jWv2ygVT5uzgPxxxze4/esycnc7ZV5aIiFRW2deWnHb6Ytfs0aFRVz6Tt69nKRVzrtnF8ow5G4/63ntPTdlnZ0O+x8eCKwUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACjz7qOxEd9unenqgjm7e89O1+ziZMycbaq176cREVm99Axztr6l2zV70+/+aM4ePeLb85Ka69uV1LWw05z9hNh3yIiIHBsatGdHDrtmHxjYbc7Om7fcNbuUL7vyrY5zK13+s2t2PpMyZ6+/4cuu2f0D9l1WL+7a45r98l7749N/qNc1u6qyypVvbWo3Z/NV9p1aIiLNbS3mbGbK91wejdqfbxNZ+44sK64UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACjzmotMpuga3NpYbc72vnzANXu8YF9HcErzaa7ZHa32fCxZ45rdErd/TL9ttv0j+iIioWjCdyxz7PPn1PjWP6Qq683ZZMS3XmBiaMCczUfs61BERJ7f6TsPO2ri5uzmXz/lmp0M7OdhZ/ds1+yl715gzn74Ix9zzR4ZGzFn9++zr9sQEXlx+4uu/K5d9pUbe3f71pDEUrXm7NRw2jW7vqnOnK2q9b1OWHClAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAAZd59NDGWdw2uXbbInB1PLnTNjneMm7Mv7Njmmh3kc+bs/pd3uGZPTY6bs+lp+24VEZHpGd/jc+byFeZsXb35NBERkVmN9t1HrQvOdM32PD61jb77sHN2syu/6vT55uxkZso12/N2bXI84xo9OTZhzkZDM67Z4Yg9u8D52J+22Jdf07/PnH1hy2LX7G27D5qzxclR1+xMzv74jE4dds224EoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAADKvNSmUCi7Bu/bs9+cfebpW12zx470m7OZtH2PiIhIXaN9/83g8DHX7MqKhDkbTi1wzS6EfHt+Bsez5mwyFrhm52XEnp3d7Zo9NDxozqade3simZIrX1hoP/aw+I4lCGLm7NjEuGv28JD9+dPZ4TsPU1U15mx6wrcTKJG0P39ERGrr7Mey9N3LXLPHpu37wHYcG3LNzkXt79Vncr7ZFlwpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFDmz2pH4yHX4MlR+zqCUGnaNTsveXM2Fku6ZieSleZsQ32Ta3bTKS3m7ES+yjU7W/DdzsrKCns4nHPNllLBHI0535YkY3FzNh61r4oQERkd861dyM3Yz8NCwff8yebtz4nzz32/a3Z6csycPTZkfx6LiGSm7etTgrJvfcq04/4WEZkYta+AOHqwzzW7s7vdnN25I+WanRmfNGerHE9jK64UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgzLuPwmHf7paIY+9MPGE+DBERqSzad6ZkDk24Zs/k7PtV0hNp1+y4Y1HJTKzZNTuI+e7DQsmencnbdxmJiIhn5VDEcSAikinbjyWSn3HNjhbLrnx22n4s5bBvdiZrP2+3Pve0a3bEcZ9Xxn17eypiNeZsOe7bTRUO+97DtrSeYs6Wyr7Xt8N9e83ZTNq+y0hEJOp4PUw2+x4fC64UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACjz56mLOftqCRGRmUjRnJ0q+dZFREL2LguJ77gDR08GZd/s4cOHzdnqtk7X7KhzBUAykbTPjk27ZntWNBTzWddsz7aVQi7nml3y7P4Qkd5d+8zZ6Wnfmotc3n4sDz/wM9fs9etXm7ORlkbX7IkD/eZsWOwrMURE4qlKV76YsGcbG9tds//8h+fM2YG9O12z65vtK25qK32rQiy4UgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgDLvPpKib89PqWTPx6O1rtmRwD57OuHb2yOeHUKhiGt03rHrpeRbCSRB1nc7y2X7bqqKSvueJBGRwZ5Rc/aU4RHX7KpoypX3CDl2aomI9P7rdnM2E427Zk+G8+Zsqcq3n+jA0Iw529Xl20800mvfCVRV7ds3FE12uPKlcfvuq5xzN9WSFfb9UWcdPuiaPXZk2JxtzDtf3wy4UgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgzGsuQuWCa3Bu2v4R83KszjU75FgvEY371gtUVVfZjyPwfTS+tmWBPdve6Zodjflu57z5bebsBe9f5prdXPkRc3Z40nde5Usxc7a6vto1e+rAfle+0D9kzk6O+1Yd7NjytDk7p9X+WIqIRHfbV2jsD+zrUERExnr7zdlcg/04RERC9b6VG9NTk+ZsTVOza3ZHu33lxorl57tmv/zis+ZsT+Yl12wLrhQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKDMu4/mnfZu1+BQyb7XZHLGtwMlGUuYs+1nzHPNrq2vM2dn8vb9TiIiVTUt5mx9Rck1u7LgyyeKh83ZVG+rb3YiMGeLI8O+2TWN5mz5sO/xGe/z7SdKBDPmbDTnO5bo8Ig5m6327QQqv9xnzo488yfX7EJjvTkbLdjPExGRw/X24xYRGTxkfzzfc877XbP3/OoJ+3GU7a9XIiJlx3nVtfg012wLrhQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKDMu4/K8ZRr8KxkhTnbki+7Zh8dO2bO9vXvdc3O77PvqCnm7TtKRERiQxlzdlHEtxcmmY+48o2z283ZioG0a3Z/yL7r5Wg865pd2d1hzkZ67eeJiMh42v74iIjMrm0zZ+MZ37nS1WLfk5VttmdFRHIjk+ZsLDftml0I7Oft8PBR3+wB346nw7m4OTvrmWdcs6eP2M+tdGu3a3ZxatScXdD6PtdsC64UAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAACjzmou6rne7Bo/85kFzNt3zomv2/lSlOVvVWOeaXd9gzyfj9o/Ri4g0jB0xZ9+92LdWZP+Ib81FOG9f51HuqnbNDiRkzhZHfGsUZqbz5mxtwXefVNX67vNQc8ycLeeTrtkHBuznSl7sxyEiUj9uX3ORbfY99jP19udEqMW+JkREZPOuHa78SMZ+jreferprdm17qzmbiPsen1LOvvanVLY/16y4UgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgDLvPooFRddgz7qPOc21rtmTUmXORqsbXbPDIXtPlsozrtlDdfadTb+abnDNfteKZa58/ZxTzdnRnj2u2bUV9n05zXO6XLOnKuz7ieo+9reu2Q1i3zkjIhLkx8zZYpc9KyJSO3nMnM2NjbtmTx6wH0sy7dvvVSzad1m1zvjuk9jwiO9YSgVztr/nBdfsyZEac7brPetcs/els+bs/p4+12wLrhQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKPOai/zEhGtwtqrZnD04mXfNDhL2HRqZ6Yxrdi5rX3VQKOZcs0PRkDmbqrGvcxARqX7fRb5jqbKv3Mj//T+6Zk/U2teWpGrt54mIyHhm0pwt33eva3bCs5tFRCLDw+Zsti7hmv3F628wZ2/9n991za6ocKyJKftWf5Rq7edtULCvxBAReVdDvSufdryupJxvj8cGjpizhZ4e1+xCaMqcHT866JptwZUCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAACUeffRwSP9rsH1tfb9KsXhrGt2KGLfIxMt+na3FMslczYWjrhm19TVmLPJlG/30aF/+B+ufFPZvuenVmZcs8PN9tv55/19rtlPbHnSnL0sUu2a3VBt3wclIpKbsZ9bQUOLa/aTT/3WnH1x737X7FAyac62xH3neMcH1pqziabZrtktRwZc+divnjBn6895n2v20d/Yz8Np5/61lnCFOdve8ea/r+dKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICyr7nY9TvX4Fh7szk7e95prtkD/b3mbCFXdM0uBfbVBeGQr1NrZ3eas4uXne6aPbHhEVc+VLbfzmjYvvpDRKQ8njZnB+P2dSgiIrO6zjRn+0YPu2anZ3zrPOZG7GsxsiHf7FDB/vi0t81xzd4z8LI5m633PT5Vz7xgziaP/LNr9tG8bx3OxOCwOdv/8M9cs095z0pzdnAq55o9dKxgzs4t+tbhWHClAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAAZd59NLuj0TW4scq+M6WjbYFr9vjgQXM2mwm5ZodDjt1HSd9OoPzBvebsnv32/TQiIqnxKVe+qqLCnN3XWO+afWjA/vhMV/hmxxy7kvZUOvcq+U4VOWXKfp8nQknX7J7D9r1NwYxvt04oGTdny9VNrtm5ov0+aUr49pJVFgNXPp6MmLNV1dWu2VNHR+yz5851za6bO9+cDcKTrtkWXCkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUOY1F9mC76P0B1+2rzro37rVNbtigX3lRv0s38fXy9P2FQD5gv1j9CIiM6G8OZuZtmdFRJLtba58ZVurOdvf2O6aPT4wZM6W7FtFRERkSad9dcVLe2Ku2RWBb89FrL7WnC00NLtmZ9Npc7al3beCpmrEvi6iWPKd4/0x+zqP/kbzy4+IiBzNHnHlczP227m94Fu5kZi0Pz5djuMQEZk3d5Z99uxFrtkWXCkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAECZl4949oiIiHQuepc9nB13zS5E7PuJiknfDqFMzn47g6JvdrHGvtMkuXSZa/bfrLDv4RERGTk4bs6G0ynX7HgyY86GQtOu2Z2LlpqzPf1HXbNntcxx5Q8G9mM/NO27nYO9A+bskuUrXLN379ppzqazvuMuz9h3CMUrXKNlXvdcVz42ZT8POxbbn5siInt2HjBnF8Ttr1ciIn/uHTdnx8bsO5hERD7y4dfPcKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQFEKAABl3n00PT7hGjxvzXL7QSS7XbO3b99hzpbyIdfshuY2c7YqMuKanUxU2sPxcdfsiphv99Hg0LA5W1/rm92yZJ45+9KOl1yzc/kZc7YwNeWavSvY78q/vHe3OTsxOu6anXDsy6lrP9U1e8mixeZsf6/9NoqI7O09aM7+t898yjU7mapz5X8ystGc/egHPuiafeuuH5izB/v+7Jr937/8eXO2panVNduCKwUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAyrzmYmxw0DX4QM9Rc7brtE7X7MYm82FLf1/WNTtRWzJnTz/zDNfs3l77ffLSru2u2aUg6cqLY/3H1MvPuEZPB4E5m8nY11aIiExn7flUlX1VhIjI0UM7XfkrLjrXnH3w4d+4ZhckYs5Wp3yP/UzGvp4lmvC9b5wYz5izP/n5o67Zuem8K3/s6Jg5e//9/+iafdGH/9acXbp0iWt2c2OTOVuRqnLNtuBKAQCgKAUAgKIUAACKUgAAKEoBAKAoBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAyrxEqKOr2zV4LJMzZ0M9h1yzx0fss3v2HXTNfmnXfnN201Ou0SJB2RwtlO1ZEZEXX9znyqfs66NkfMq+Q8Zr8ZJlrnx1tX0vTF1ztWv2Ry76lCs/NDhkzgaBfaeWiEh+2r6za9+uba7Zfb3250Qs6jvuZUtXmrOzZ7e4ZieTCVd+VmubOXvpxz7smt09176vLVS277ESEZlyPPaFQsE124IrBQCAohQAAIpSAAAoSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAADKvOxg+eqzXYOzWfsqikgs7ppdKfb1BZG+Xtfs7Oi4ORsKuUZLOByzZ0NF1+xoeMaVT1TUmrOnzl7imj0xPmnOTk3lfbOHD5uz//rMLtfsTU8958pnJjPmbCzme3yamurM2fZZla7Zl19mX+exZPEi1+zOzjnmbCLuW1sRjTl2s4hIPJY0Z0tl3/NteHjUHvZtChEJOVbchJ0vQpaRb/pEAMB/WpQCAEBRCgAARSkAABSlAABQlAIAQFEKAABFKQAAFKUAAFCUAgBAUQoAAGVeJpKZtO+zEREplSLm7OjIgGt2LpM1Z1uaWlyzx4eHzdlSybGjRETKJfs+qLrmVtfsWe3drnwQsu+RyeftO35EREaH95mzhcIh1+z/9aO/N2cr4vZzUERk4anzXfnVq5fbsyuXuWYvXnyaOdvW6jvHo46dQ9M5386mbNZ+rhSLgWt2xvG8FxEZn5kyZ6NR316lWMyxxyzuW34Ui1aYs9GIb2+cBVcKAABFKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAAFQoCALTZ80XnbHCNTidHjdnZzXZP9YtInJ44Jg529ziWwHQs7/XnM1mp12zkxUpc7aissY1u1D0rXSIOD7V39pY5Zrd1dlhzra1+R6fhUvONGfPWrXSNbu9fZYrn6pOmrPRsD0rIjJTKJqz+WnfeZjP29culCXkmh2N2vNB4JstId9amXDY/pwIh33vjyNhxxPIeTNLRftjXyrmXbNPXbj4dTNcKQAAFKUAAFCUAgBAUQoAAEUpAAAUpQAAUJQCAEBRCgAARSkAABSlAABQlAIAQJl3HwEA/uvjSgEAoCgFAICiFAAAilIAAChKAQCgKAUAgKIUAACKUgAAKEoBAKD+D3WdmxFDMSF+AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CompactVGG11_CIFAR10().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8, 12], gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "val_loss, val_acc = evaluate(model, testloader, criterion)\n"
      ],
      "metadata": {
        "id": "w1JfMiig0gbO"
      },
      "id": "w1JfMiig0gbO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_acc)"
      ],
      "metadata": {
        "id": "hzCmghw81j6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2534ff-b41c-4fed-f954-3bf87c0332ce"
      },
      "id": "hzCmghw81j6M",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}